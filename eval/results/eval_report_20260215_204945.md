# RAG Evaluation Report

**Date**: 2026-02-16T01:49:45.637157+00:00

## Aggregate Metrics

| Metric | Value |
|--------|-------|
| Total Queries | 22 |
| Evaluated | 21 |
| Skipped | 1 |
| Avg Groundedness | 0.6349 |
| Avg Citation Precision | 0.6865 |
| Avg Confidence | 0.4365 |
| Failure Cases | 11 |

## Per-Query Results

### Direct Queries

| ID | Groundedness | Citation Prec. | Confidence | Answer Len |
|----|-------------|----------------|-----------|-----------|
| d01 | 1.000 | 1.000 | 0.500 (abstained) | 43 |
| d02 | 0.615 | 0.333 | 0.503 (medium) | 1015 |
| d03 | 1.000 | 1.000 | 0.500 (abstained) | 43 |
| d04 | 0.609 | 1.000 | 0.765 (high) | 2993 |
| d05 | 1.000 | 1.000 | 0.500 (abstained) | 334 |
| d06 | 0.000 | 0.333 | 0.133 (low) | 635 |
| d07 | 1.000 | 1.000 | 0.500 (abstained) | 43 |
| d08 | 0.000 | 1.000 | 0.400 (medium) | 779 |
| d09 | 1.000 | 0.000 | 0.500 (abstained) | 1815 |
| d10 | 0.444 | 0.000 | 0.267 (medium) | 901 |

### Synthesis Queries

| ID | Groundedness | Citation Prec. | Confidence | Answer Len |
|----|-------------|----------------|-----------|-----------|
| s01 | 0.333 | 0.500 | 0.400 (medium) | 1804 |
| s02 | 0.200 | 0.000 | 0.120 (low) | 1686 |
| s03 | 0.429 | 1.000 | 0.657 (medium) | 1676 |
| s04 | 1.000 | 1.000 | 0.500 (abstained) | 43 |
| s05 | 0.278 | 0.250 | 0.267 (low) | 1936 |

### Edge Case Queries

| ID | Groundedness | Citation Prec. | Confidence | Answer Len |
|----|-------------|----------------|-----------|-----------|
| e01 | 0.000 | 0.000 | 0.000 (low) | 829 |
| e02 | 1.000 | 1.000 | 0.500 (abstained) | 202 |
| e03 | 1.000 | 1.000 | 0.500 (abstained) | 156 |
| e04 | 1.000 | 1.000 | 0.500 (abstained) | 43 |
| e05 | 1.000 | 1.000 | 0.500 (abstained) | 43 |

### Stress Test Queries

| ID | Groundedness | Citation Prec. | Confidence | Answer Len |
|----|-------------|----------------|-----------|-----------|
| x01 | 0.425 | 1.000 | 0.655 (medium) | 3596 |
| x02 | SKIPPED | — | — | — |

## Representative Failure Cases

### Failure 1: d02

**Query**: How do AI code-generation tools affect developer productivity according to the corpus?

**Groundedness**: 0.6154
**Citation Precision**: 0.3333

**Ungrounded sentences**:
> These tools automate repetitive tasks, such as debugging, code refactoring, and security vulnerability detection, allowing developers to focus more on problem-solving and innovation [ijnrd_2024, ijnrd_2024_chunk_0016].
> ## References
- IJNRD Author(s).

**Answer snippet**: AI code-generation tools significantly reduce development time and improve software quality by generating contextually relevant code snippets, thereby enhancing developer productivity [ijnrd_2024, ijnrd_2024_chunk_0087]. These tools automate repetitive tasks, such as debugging, code refactoring, and…

### Failure 2: d06

**Query**: What security concerns are raised about AI-generated code in the literature?

**Groundedness**: 0.0
**Citation Precision**: 0.3333

**Ungrounded sentences**:
> Security concerns about AI-generated code include the potential for generating insecure or vulnerable code [arxiv_2502_08108_chunk_0015], recommending weak encryption algorithms [ijnrd_2024_chunk_0074], and introducing security flaws due to insecure code suggestions [ijnrd_2024_chunk_0069].
> Additionally, there is a risk of exposure to AI-generated malware [ijnrd_2024_chunk_0069], and the variability in AI outputs can lead to security vulnerabilities over time [arxiv_2502_08108_chunk_0015].

**Answer snippet**: Security concerns about AI-generated code include the potential for generating insecure or vulnerable code [arxiv_2502_08108_chunk_0015], recommending weak encryption algorithms [ijnrd_2024_chunk_0074], and introducing security flaws due to insecure code suggestions [ijnrd_2024_chunk_0069]. Addition…

### Failure 3: d08

**Query**: How is model-driven software engineering defined in the context of AI augmentation?

**Groundedness**: 0.0
**Citation Precision**: 1.0

**Ungrounded sentences**:
> Model-driven software engineering (MDSE) is defined as a practice where models are original artifacts that are engineered with the intention of facilitating the top-down construction of complex software.
> Models are used during runtime to enable monitoring, verification, and validation of software operations, as well as the optimization of its performance.

**Answer snippet**: Model-driven software engineering (MDSE) is defined as a practice where models are original artifacts that are engineered with the intention of facilitating the top-down construction of complex software. Models are used during runtime to enable monitoring, verification, and validation of software op…

### Failure 4: d09

**Query**: What metrics are used to evaluate the quality of AI-generated code?

**Groundedness**: 1.0
**Citation Precision**: 0.0

**Answer snippet**: No sufficient evidence found in the corpus.

## References
- IJNRD Author(s), 2024. AI-Augmented Software Development: Enhancing Code Quality and Developer Productivity. International Journal of Novel Research and Development. https://www.ijnrd.org/ [ijnrd_2024, ijnrd_2024_chunk_0041]
- IJSRA Author…

### Failure 5: d10

**Query**: What are the main adoption barriers for AI tools in software teams?

**Groundedness**: 0.4444
**Citation Precision**: 0.0

**Ungrounded sentences**:
> Additionally, there is a concern that overreliance on AI tools may lead to a degradation of fundamental coding skills among developers, reducing their ability to critically analyze and solve problems.
> This can result in a lack of critical thinking and an increased risk of errors in the codebase.

**Answer snippet**: The main adoption barriers for AI tools in software teams include reliance on AI technology, ethical and security issues, technological lapses, and the proliferation of errors. Additionally, there is a concern that overreliance on AI tools may lead to a degradation of fundamental coding skills among…

## Interpretation

- **Groundedness** measures what fraction of answer sentences are supported by at least one retrieved chunk (word-overlap heuristic, threshold=0.15).
- **Citation Precision** measures what fraction of inline `[source_id, chunk_id]` citations point to chunks that actually support the enclosing sentence.
- **Confidence / evidence-strength** is a per-response score in [0, 1]: 0.6 × groundedness + 0.4 × citation precision. Correct abstentions ("No sufficient evidence") score 0.5. Tier: high (both ≥ 0.6), medium (either ≥ 0.4), low, or abstained.
- Failure cases highlight queries where either metric fell below 0.5, indicating the model may have hallucinated or cited irrelevant chunks.
